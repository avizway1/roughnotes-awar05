K8s Primary Components: 

K8s 	--> K8s
O11y	--> Observability

CONTROL PLANE COMPONENTS: (BRAIN)

API Server (kube-apiserver) : Entry point to our Kubernetes.. Front door of K8s.. It handles all the requests from the users and the other tools..

--> kubectl apply -f manifest.yaml	--> It communicates with "api server".. API server validates.. 

--> (kubectl get pods) 
API server receives the request (kubectl) and retives the data from etcd and return with pods list.

-------

etcd (key-value store) : Stores all the cluster data/metadata (configuration, deployment, state)

Its like a database, Contains cluster data (pod status, deployments, configuration)
--> etcd will have our cluster desired info..
--> Maintans the cluster desired and current state.. 

--> kubectl apply -f app.yaml (2 pods) (Api server takes the req, etcd stores the desired state)

-------

Controller Manager (kube-controller-manager) : Ensure desired state of the cluster (Replicas count, failed pods, node health)

--> It checks "etcd" and ensures the cluster matches the "desired state"..
--> CM has diff controllers:
	Node Controller : Monitors the node health, It can replace the failed nodes..
	Job Controller : Looks for job objects, Create pods to run those tasks..
	Service Account Controller : manages the authentication tokens..

---

Scheduler (Kube-Scheduler) : Decides where our workload i.e; pods should run, It chooses nodes based on the resources availability.. 

--> Whenever we are creating any workload, THis chooses the node based on resource availability and tolerations.

---

Cloud Controller Manager / CCM : Helps to integrate K8s with cloud service providers and its resources i.e; vpc, load balancer, storage..


---
---

WORKER NODE : (Actual Work happens)

A worker node is where the actual application runs. 

--> Control Plane managers the clusters
--> Worker nodes executes the workload.


kubelet : Like an agent, runs on every worker node.
--> Talks to the API server and maintains/runs the pod properly.
--> Contineously monitors the health of the pods and reports back to the control plane (controller manager)

When scheduler assign a pod to a worker, Kubelet receives the instructions, It uses Container runtime and run the workload.. If pod fails, It restarts is automatically...

----

Container Runtime : helps to run containers

--> We need it, if we want to pull an image, running a container or maintaining container lifecylce.
--> Docker, Containerd, CRIO

----

Kube-proxy : Networking component

--> Handles the communication between pods.
--> Helps to assign a unique IP address for every pods runs on the node.
--> Helps to perform load balancing between pods with in a service.

---

POD : Smallest deployable unit in K8s..

Pod is nothing but a container or multiple containers wrapped around.
Every pod will have a unique IP address with the cluster.


-----

Container : Standalone exucutable software unit that contains everything nrequired to run an application i.e; code, system libraries, dependecies, runtime...


---

ReplicaSet / RS : It helps to run our pods with required identical copies. Due to any issue if any pod goes down, it creates another pod automatically.
--> Maintains desired number of pod replicas.
--> Automatically creates another copy of the pod, if an existing pod is deleted/crashed.
--> We do have option to manually adjust the replicas.
--> Replicaset dont support rolling updates. 
kubectl get rs
kubectl get replicaset

---

Deployment : Higher level abstraction of Replicaset.
--> It supports Rolling updates and Scaling the resources without downtime.

Version 1 --> version 2 --> we can upgrade without much downtime.

--> helps us to have fixed number of replicas (Depends on ReplicaSet)
--> For Zero Downtime, it defaultly uses Rolling Updates
--> We can RollBack if deployment fails.

---

Rolling Update : helps us to migrate an app/.pod from one version to another version with zero downtime.

MaxUnavailable : 1 (Max of 1 pod can be terminated during th upgrade)
MaxSurge : 2 (2 extra pods can be created temporarily)

---

Cluster IP : It exposes the service on a cluster Internal IP. Service works within the cluster..
--> Frontend application taliking to internal backend application.

NodePort : Expositing our application outside using Node/ec2-instance public IP.

LoadBalancer : Exposing application using a dedicated LB to outside world.


----

eksctl create cluster --name=ekswithavinash --version 1.33 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type t3.small --nodes 2 --node-ami-family=AmazonLinux2023 --managed



eksctl create cluster --name=ekswithavinash --version 1.33 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type c7i-flex.large --nodes 3 --node-ami-family=AmazonLinux2023 --managed

c7i-flex.large



eksctl create cluster --name=ekswithavinash --version 1.33 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type t3.small --nodes 1 --node-ami-family=AmazonLinux2023 --managed


OIDC : IAM OpenID COnnect : This allows AWS IAM to authenticate with K8s serviceaccounts, IAM permisisons and roles.


aws eks describe-cluster --name ekswithavinash --query "cluster.identity.oidc.issuer"

If above command returns nothing, provide OIDC provider approval.. 

'''
eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster ekswithavinash --approve
'''

eksctl delete cluster --name ekswithavinash


--> To access EKS from another machine..
aws eks update-kubeconfig --region ap-south-1 --name ekswithavinash




---

kubectl get all
kubectl get pods
kubectl get rs
kubectl get deployment

kubectl get svc
kubectl get service
kubectl describe svc cal-service

kubectl describe pod <pod-name>


--

Control Plane
 - api server
 - etcd
 - kube scheduler
 - control manager
 - cloud controller manager
 
Worker Node
 - kubelet
 - kubeproxy
 - container runtime

Node
pods
Container
replicaset
deployment
services
 -ClusterIP
 -NodePort
 -loadBalancer

Whenever we are using AWS EKS, AWS manages the complete "Control PLane" components, AWS cost us "0.10$/Hr".. 
Standard Support : 0.10$/hr
Extended Support : 0.60$/hr

AWS runs 3 copies of our K8s control plane across 3 Different AZs with in the Region.
If 1 AZ goes down, no operational effect on our resources..
AWS handles the scaling, patching and Disaster recovery for the master nodes/Control plane.

---

1. AWS EC2 Self Managed Node Groups : Customer/we/you need to provision, configure and manage ec2 instances for running our workload/Worker Nodes. 
--> Customer has to take care scaling, patching OS security and updates.
--> You need to choose specific approved AMIs.

2. AWS EC2 managed Node Groups : AWS manages the Creation, configuration of the worker nodes.
--> AWS takes careos patching, updates, scaliing..
--> AWS uses "Amazon Linux 2 (till v1.32)", "amazonLinux 2023 (from v1.33)" and "Bottlerocket" as AMI.
--> Whenever we are launching workload, we need to choose the "Instance Type"

3. Fargate : Serverless - No need to manage ec2 instances. AWS automatically provision the pods, We dont get any nodes. Pay-per-usage. No need to worry about OS patching. 

4. Auto Mode : Cluster infrastructure managed by AWS includes many Kubernetes capabilities as core components, as opposed to add-ons, such as compute autoscaling, pod and service networking, application load balancing, cluster DNS, block storage, and GPU support.
--> AWS chooses the configuration automatically, scaled the workload automatically.
--> if you choose AutoMode, AWS cost us very very less for the configurqations we select.


---

Pre-Requisites to Launch EKS cluster.

1. AWS CLI, COnfigure CLI user with Admin Access
2. install EKSCTL
3. Install Kubectl

---



kubectl cluster-info				--> proviodes the cluster info
kubectl cluster-info dump
kubectl get nodes					--> List the nodes associated with cluster
kubectl describe node ip-192-168-3-19.ap-south-1.compute.internal

kubectl get pods

kubectl get all -A					--> Displays all the resources from all namespaces

kubectl get pods,services

kubectl get service,deployment -A

kubectl logs pod/cal-deployment-5dbcc68c56-n5gw5		--> Displays the logs from a pod

kubectl exec -it pod/cal-deployment-5dbcc68c56-n5gw5 -- /bin/sh		--> Connect to a pod

kubectl delete pod/svc/rs <name>									--> To delete the resource

kubectl deploy -f <filename>.yml				--> Do the deployment

kubectl scale deployment cal-deployment --replicas=2			--> we can adjust the deployment properties using command line

kubectl apply -f 03-deployment.yml --dry-run=server 			--> helps to perform dryrun

kubectl top nodes 					--> Displays the resources usage (CPU, Memory)

kubectl port-forward <pod-name> <laptop-port>:<pod-port>		

kubectl get events					--> Displays the cluster events info 

---

Namespace : Helps to logically isolate or seperate the resources in K8s..
--> Multi-Tenant : Multiple teams can use only one cluster without interfering other teams work..
--> Resource limits: we cna setup resource quotas for better workload management..


kubectl get ns

kubectl create ns dev-namespace 			--> Create a Namespace

NAME              STATUS   AGE
default --> our workload runs here. If we run anything without specifying ns explicitly.
kube-node-lease --> manages the node ATT/heartbeat to track node availability.
kube-public --> Publicly readable namespace used for cluster-wide information..
kube-system --> K8s backend/support core/system components runs here.. i.e; CoreDNS, kube-proxy, monitoring..


kubectl get all -n kube-system		--> Display resources from a specific namespace


---

limitranege : Helps to setup the resource limits/quotas.

---

DaemonSet : It helps/ensures to run a specif pod on every node we have in our K8s workload. 

--> One pod runs per node 
--> We can use Node Selectors to run these daemonsets only on specific nodes.
--> When we delete a node, pod also deletes


EKS --> 3 nodes (3 Pods runs across 3 nodes)
EKS --> 2 nodes (2 Pods runs across 2 nodes)

--> Log collections (prometheous, Fluent Bit and Filebeat)
--> Monitoring agents (dynatrace, site24x7, middleware.io)
--> network Plugins required on all Nodes

---

kubectl get nodes
ip-192-168-18-246.ap-south-1.compute.internal   Ready    <none>   10m   v1.33.5-eks-113cf36

kubectl get nodes -o wide

NAME                                            STATUS   ROLES    AGE   VERSION               INTERNAL-IP      EXTERNAL-IP      OS-IMAGE                       KERNEL-VERSION                   CONTAINER-RUNTIME
ip-192-168-18-246.ap-south-1.compute.internal   Ready    <none>   10m   v1.33.5-eks-113cf36   192.168.18.246   15.206.153.204   Amazon Linux 2023.9.20251027   6.12.53-69.119.amzn2023.x86_64   containerd://2.1.4

---

kubectl label node <node-name> key=value
kubectl label node <node-name> key=value

====

Stetefulset : 

--> We will have persistent storage 
--> name and network will be persistent
--> Stable storage for the pods
--> it will be scaled (creation/deletion) in order

==> Statefulsets required Headless service..

--> headless service is a special kind of service without ClusterIP. It does not distribute any load across the pods. It can directly expose the individual pods.


==> Step 1 : 

cluster_name=ekswithavinash

eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster $cluster_name --approve

--

Step 2

grab the EBSCSI Driver policy ARN : arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy

eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster $cluster_name --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --approve --role-only --role-name AmazonEKS_EBS_CSI_DriverRole

Copy the RoleARN:
arn:aws:iam::982424467695:role/AmazonEKS_EBS_CSI_DriverRole


export SERVICE-ACCOUNT_EBS=arn:aws:iam::982424467695:role/AmazonEKS_EBS_CSI_DriverRole

---

Step 3 : Install EBS CSI Driver add-on

eksctl create addon --name aws-ebs-csi-driver --cluster $cluster_name --service-account-role-arn arn:aws:iam::982424467695:role/AmazonEKS_EBS_CSI_DriverRole --force

--
Step 4: validate

kubectl get daemonsets.apps ebs-csi-node -n kube-system
kubectl get pods -A

=====

configMap : Equalant to "AWS SSM parameter Store"
Secrets: Equalant to "AWS Secrets Manager"


Create a file with al your sensitive info

kubectl create configmap db-config --from-env-file=config.txt

---

kubectl create secret generic db-secret --from-file=dbpassword.txt


--

kubectl get secret db-secret -o jsonpath='{.data.DB_PASSWORD}' | base64 -d

---

helm charts: 

install
uninstall / delete
history
versioning


My-app/
	Charts
	Templates
		deployment.yml
		service.yml
		ingress.yml
		HPA.yml
		Serviceaccount.yml
	values.yml
	chart.yml
	
		
Install helm

https://artifacthub.io/


helm ls				--> displays charts deployed using helm
helm repo ls		--> Lists installed charts ion current machine

helm search repo bitnami		--> List out all bitnami's repos
helm search repo bitnami/cassandra	--> Displays info about bitnami's cassandra chart

helm install my-nginx bitnami/nginx

helm create mynodeapp


helm rollback mynodeapp 4


HPA : Horizontal Pod Autoscaling : 


Vertical Scaling : Ading more resources i.e; cpu, memory to an existing instance.
Horizontal Scaling : Adding more instances/servers to deliver our application.


VPA : Vertical pod Autoscaling : We dont use.. 
HPA : Horizontal Pod Autoscaling : 

if we are getting more load on existig pods, AWS HPA automatically add more pods/ remore existing pods.

--> Adding pods happens very quickly
--> Removing pods happens very slowly

We can use CPU, memory to scale the resources.

kubectl autoscale deployment cal-app --cpu-percent=50 --min=1 ==max=10

kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=6


kubectl get hpa



kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"



CPU
===============
m --> millocores

0.1 CPU --> 100m
0.5 CPU --> 500m
1 CPU --> 1000m
2 CPU --> 2000m

Memory
==============
Mi --> mebibytes
Gi --> Gibibytes

1 Mi = 1024 KiB
1 Gi = 1024 Mi


====


eksctl create cluster --name=ekswithavinash --version 1.33 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type t3.small --nodes 2 --node-ami-family=AmazonLinux2023 --managed

eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster ekswithavinash --approve

aws eks describe-nodegroup --cluster-name ekswithavinash --nodegroup-name ng-default


helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm repo update
helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set autoDiscovery.clusterName=ekswithavinash \
  --set awsRegion=ap-south-1 \
  --set extraArgs.balance-similar-node-groups=true \
  --set extraArgs.skip-nodes-with-system-pods=false



---

eksctl create iamserviceaccount   \
   --name ebs-csi-controller-sa   \
   --namespace kube-system   \
   --cluster ekswithavinash  \
   --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy  \
   --approve \
   --role-only \
   --role-name AmazonEKS_EBS_CSI_DriverRole

export SERVICE_ACCOUNT_ROLE_ARN=arn:aws:iam::982424467695:role/AmazonEKS_EBS_CSI_DriverRole


aws ecr-public get-login-password --region us-east-1 | helm registry login --username AWS --password-stdin public.ecr.aws


---

eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster ekswithavinash --approve

---

Rolling Update : Default deployment strategy K8s provides.. 
Maxunavailable : 1
MaxSurge : 2

--> if we dont define Maxunavailable/max surge.. It picks 25% as default..

kubectl apply -f <name>.yml
kubectl set image deployment\rolling-deploy rollingapp=nginx:1.20
kubectl rollout status deployment/rolling-deploy
---

Recreate Deployment : old pods will be killed and new pods are created. It causes downtime but its simple and clean. We dont use this in production environment as it causes some downtime.

---

Blue / green Deployment : 
We have to run 2 seperate/different environments.. (blue --> current.. Green --> new)
--> Safest deployment method.. You need extra resources 

---

Canary Deployment : Gradually sens traffic to newer version in phased manner i.e; 10%, 30%, 40%, 50%..
This required multiple dfeployments or service controllers..


=====================


Affinities : We can control where the pod should run..

Pod Affinity : Assume, we have 2 pods for our application.. If we implement pod affinity, both the pods runs together (on same node).. 

Node Affinity : Assume, your workload is related to DB and this should run in a node labelled as "db".. 

Pod anti-affinity : Pods will span/spread across nodes.. both pods wont run on same node..


--> node affinity

kubectl label nodes ip-192-168-32-231.ap-south-1.compute.internal disktype=ssd


---

https://github.com/GoogleCloudPlatform/kubectl-ai


---

Taint (Node): Like a bus seat marked "Reserved".
Toleration (Pod): The special pass that lets you sit there. ðŸ‘‰ Without the pass, you cannot sit on the reserved seat.


Step 1 : Add a label to a "node", so that we can force our pod to run on that specific node using "NodeSelector"

Step 2 : Taint the same node

kubectl taint nodes ip-192-168-58-148.ap-south-1.compute.internal role=db:NoSchedule

kubectl taint nodes ip-192-168-58-148.ap-south-1.compute.internal role=db:NoSchedule-


step 3 : Deploy the pod with tolarations.. and it should launch on the tainted instance only..

Step 4 ; Deploy the pod without tolaration, and enforce it to run on the same node.. It should fail..


Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  48s   default-scheduler  0/2 nodes are available: 1 node(s) didn't match Pod's node affinity/selector, 1 node(s) had untolerated taint {role: db}. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling

---

KUBERNETES HEALTH PROBES DEMONSTRATION
 
THREE TYPES OF PROBES:
1. STARTUP PROBE: Checks if container has started successfully
2. READINESS PROBE: Checks if container is ready to receive traffic
3. LIVENESS PROBE: Checks if container is still running healthy

---

minReadySeconds: Delay before a pod is considered available after becoming ready (helps avoid sending traffic too soon).
progressDeadlineSeconds: Max time for a rollout to complete before it's considered failed.
terminationGracePeriodSeconds: Time for a pod to shut down gracefully before being killed.

---

PDB : Pod Distruption Budget : Its a K8s settings to tell the system how many pods of our application "must stay running" when something happens i.e; node upgrade or node maintenance window/operations..


Purpose: Ensures only a limited number of pods are down during voluntary disruptions (e.g., node drain, upgrades).


--> minAvaialable : 
--> maxUnavailable : 


kubectl drain <node-name> --ignore-daemonsets

kubectl drain ip-192-168-29-249.ap-south-1.compute.internal --ignore-daemonsets

kubectl uncordon ip-192-168-29-249.ap-south-1.compute.internal

----

container
pod
replica set
deployment (Rolling update, blue green, shadow, canary, recreate)
service	(ClusterIP, NodePort, LB, headless)
namespace
daemonsets
statefulsets
configmaps
secrets
hpa
vpa
cluster auto scaler
goldilocks
helm
oidc
servcie account
ebs csi drivers
affinities
tains and tolerations
pdb
probes
ingress control
istio
promotheous
grafana
slack integrations
auto mode
fargate
kops


===

Step 1 : Create cluster and OIDC approval

Step 2 : Install load balancer controller

Download the policy document and create an iam policy in aws account, name it as "AWSLoadBalancerControllerIAMPolicy"

Now, Grab the policy ARN : arn:aws:iam::982424467695:policy/AWSLoadBalancerControllerIAMPolicy


Step 3 : create a service acocunt for load balancer controller

eksctl create iamserviceaccount \
    --cluster=ekswithavinash \
    --namespace=kube-system \
    --name=aws-load-balancer-controller \
    --attach-policy-arn=arn:aws:iam::982424467695:policy/AWSLoadBalancerControllerIAMPolicy \
    --override-existing-serviceaccounts \
    --region ap-south-1 \
    --approve

verify the sa status.. : kubectl get sa -n kube-system

Step 4 : Add eks helm repo and update

helm repo add eks https://aws.github.io/eks-charts
helm repo update eks


Step 5 : Install the helm repo

helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=ekswithavinash \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller


Verify the Load Balabcer controller creation status..

kubectl get all -n kube-system


===========

https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/


====

eksctl create cluster --name=ekswithavinash --version 1.33 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type c7i-flex.large --nodes 2 --node-ami-family=AmazonLinux2023 --managed


eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster ekswithavinash --approve



===
ISTIO CONFIGURATION
===

istio Documentation page: https://istio.io/latest/docs/setup/getting-started/

Download the latest istio:
curl -L https://istio.io/downloadIstio | sh -

cd istio-1.28.0

export PATH=$PWD/bin:$PATH


istioctl install -f samples/bookinfo/demo-profile-no-gateways.yaml -y
kubectl label namespace default istio-injection=enabled

---

For testing purposes: Create a pod in default namespace, you should automatically get the sidecar container / envoy proxy..


---


kubectl get pods -n istio-system

kubectl get crd gateways.gateway.networking.k8s.io &> /dev/null || \
{ kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd?ref=v1.4.0" | kubectl apply -f -; }


kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml


kubectl get services

kubectl get pods

validate: kubectl exec "$(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')" -c ratings -- curl -sS productpage:9080/productpage | grep -o "<title>.*</title>"

Above command should display book title text.. 

===

git clone https://github.com/istio/istio.git
kubectl apply -f samples/addons

===

navigate back to istio downloaded folder..

kubectl apply -f samples/addons/kiali.yaml
kubectl rollout status deployment/kiali -n istio-system

istioctl dashboard kiali

for i in $(seq 1 100); do curl -s -o /dev/null "http://localhost:8080/productpage"; done

---
---

kubectl run curl-naked \
  -n default \
  --image=curlimages/curl:8.4.0 \
  --annotations='sidecar.istio.io/inject=false' \
  -- sleep 3600

kubectl exec -n default -it curl-naked -- \
  curl -v http://productpage:9080/productpage


---

If mTLS is STRICT â†’ curl fails (503 or connection error)

If mTLS is PERMISSIVE â†’ curl succeeds (200 OK)



---
Now apply strict mTLS

apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: default
spec:
  mtls:
    mode: STRICT


--

kubectl exec -n default -it curl-naked -- \
  curl -v http://productpage:9080/productpage

--



kubectl run curl-naked-2 \
  -n default \
  --image=curlimages/curl:8.4.0 \
  -- sleep 3600

kubectl exec -n default -it curl-naked-2 -- \
  curl -v http://productpage:9080/productpage



-----


~/.kube/config


kubectl config get-contexts
kubectl config current-context

# add Dev cluster (Cluster1)
aws eks update-kubeconfig --region <region> --name cluster1 --alias dev-cluster
# add UAT cluster (Cluster2)
aws eks update-kubeconfig --region <region> --name cluster2 --alias uat-cluster


aws eks update-kubeconfig --region ap-south-1 --name ekswithavinash --alias dev-cluster
aws eks update-kubeconfig --region ap-south-1 --name ekswithavinash-uat --alias uat-cluster


kubectl config use-context dev-cluster    # switch to Dev/Cluster1
kubectl config use-context uat-cluster    # switch to UAT/Cluster2


kubectl --context=dev-cluster get pods
kubectl --context=uat-cluster get pods


----

eksctl create cluster --name=ekswithavinash --version 1.33 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type t3.small --nodes 2 --node-ami-family=AmazonLinux2023 --managed


eksctl create cluster --name=ekswithavinash-uat --version 1.33 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type t3.small --nodes 1 --node-ami-family=AmazonLinux2023 --managed




AWSCodePipelineServiceRole-ap-south-1-deploy-to-ec2











