K8s Primary Components: 

K8s 	--> K8s
O11y	--> Observability

CONTROL PLANE COMPONENTS: (BRAIN)

API Server (kube-apiserver) : Entry point to our Kubernetes.. Front door of K8s.. It handles all the requests from the users and the other tools..

--> kubectl apply -f manifest.yaml	--> It communicates with "api server".. API server validates.. 

--> (kubectl get pods) 
API server receives the request (kubectl) and retives the data from etcd and return with pods list.

-------

etcd (key-value store) : Stores all the cluster data/metadata (configuration, deployment, state)

Its like a database, Contains cluster data (pod status, deployments, configuration)
--> etcd will have our cluster desired info..
--> Maintans the cluster desired and current state.. 

--> kubectl apply -f app.yaml (2 pods) (Api server takes the req, etcd stores the desired state)

-------

Controller Manager (kube-controller-manager) : Ensure desired state of the cluster (Replicas count, failed pods, node health)

--> It checks "etcd" and ensures the cluster matches the "desired state"..
--> CM has diff controllers:
	Node Controller : Monitors the node health, It can replace the failed nodes..
	Job Controller : Looks for job objects, Create pods to run those tasks..
	Service Account Controller : manages the authentication tokens..

---

Scheduler (Kube-Scheduler) : Decides where our workload i.e; pods should run, It chooses nodes based on the resources availability.. 

--> Whenever we are creating any workload, THis chooses the node based on resource availability and tolerations.

---

Cloud Controller Manager / CCM : Helps to integrate K8s with cloud service providers and its resources i.e; vpc, load balancer, storage..


---
---

WORKER NODE : (Actual Work happens)

A worker node is where the actual application runs. 

--> Control Plane managers the clusters
--> Worker nodes executes the workload.


kubelet : Like an agent, runs on every worker node.
--> Talks to the API server and maintains/runs the pod properly.
--> Contineously monitors the health of the pods and reports back to the control plane (controller manager)

When scheduler assign a pod to a worker, Kubelet receives the instructions, It uses Container runtime and run the workload.. If pod fails, It restarts is automatically...

----

Container Runtime : helps to run containers

--> We need it, if we want to pull an image, running a container or maintaining container lifecylce.
--> Docker, Containerd, CRIO

----

Kube-proxy : Networking component

--> Handles the communication between pods.
--> Helps to assign a unique IP address for every pods runs on the node.
--> Helps to perform load balancing between pods with in a service.

---

POD : Smallest deployable unit in K8s..

Pod is nothing but a container or multiple containers wrapped around.
Every pod will have a unique IP address with the cluster.


-----

Container : Standalone exucutable software unit that contains everything nrequired to run an application i.e; code, system libraries, dependecies, runtime...


---

ReplicaSet / RS : It helps to run our pods with required identical copies. Due to any issue if any pod goes down, it creates another pod automatically.
--> Maintains desired number of pod replicas.
--> Automatically creates another copy of the pod, if an existing pod is deleted/crashed.
--> We do have option to manually adjust the replicas.
--> Replicaset dont support rolling updates. 
kubectl get rs
kubectl get replicaset

---

Deployment : Higher level abstraction of Replicaset.
--> It supports Rolling updates and Scaling the resources without downtime.

Version 1 --> version 2 --> we can upgrade without much downtime.

--> helps us to have fixed number of replicas (Depends on ReplicaSet)
--> For Zero Downtime, it defaultly uses Rolling Updates
--> We can RollBack if deployment fails.

---

Rolling Update : helps us to migrate an app/.pod from one version to another version with zero downtime.

MaxUnavailable : 1 (Max of 1 pod can be terminated during th upgrade)
MaxSurge : 2 (2 extra pods can be created temporarily)

---

Cluster IP : It exposes the service on a cluster Internal IP. Service works within the cluster..
--> Frontend application taliking to internal backend application.

NodePort : Expositing our application outside using Node/ec2-instance public IP.

LoadBalancer : Exposing application using a dedicated LB to outside world.


----

eksctl create cluster --name=ekswithavinash --version 1.33 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type t3.small --nodes 2 --node-ami-family=AmazonLinux2023 --managed


OIDC : IAM OpenID COnnect : This allows AWS IAM to authenticate with K8s serviceaccounts, IAM permisisons and roles.


aws eks describe-cluster --name ekswithavinash --query "cluster.identity.oidc.issuer"

If above command returns nothing, provide OIDC provider approval.. 

'''
eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster ekswithavinash --approve
'''

eksctl delete cluster --name ekswithavinash

---

kubectl get all
kubectl get pods
kubectl get rs
kubectl get deployment

kubectl get svc
kubectl get service
kubectl describe svc cal-service

kubectl describe pod <pod-name>


--

Control Plane
 - api server
 - etcd
 - kube scheduler
 - control manager
 - cloud controller manager
 
Worker Node
 - kubelet
 - kubeproxy
 - container runtime

Node
pods
Container
replicaset
deployment
services
 -ClusterIP
 -NodePort
 -loadBalancer

Whenever we are using AWS EKS, AWS manages the complete "Control PLane" components, AWS cost us "0.10$/Hr".. 
Standard Support : 0.10$/hr
Extended Support : 0.60$/hr

AWS runs 3 copies of our K8s control plane across 3 Different AZs with in the Region.
If 1 AZ goes down, no operational effect on our resources..
AWS handles the scaling, patching and Disaster recovery for the master nodes/Control plane.

---

1. AWS EC2 Self Managed Node Groups : Customer/we/you need to provision, configure and manage ec2 instances for running our workload/Worker Nodes. 
--> Customer has to take care scaling, patching OS security and updates.
--> You need to choose specific approved AMIs.

2. AWS EC2 managed Node Groups : AWS manages the Creation, configuration of the worker nodes.
--> AWS takes careos patching, updates, scaliing..
--> AWS uses "Amazon Linux 2 (till v1.32)", "amazonLinux 2023 (from v1.33)" and "Bottlerocket" as AMI.
--> Whenever we are launching workload, we need to choose the "Instance Type"

3. Fargate : Serverless - No need to manage ec2 instances. AWS automatically provision the pods, We dont get any nodes. Pay-per-usage. No need to worry about OS patching. 

4. Auto Mode : Cluster infrastructure managed by AWS includes many Kubernetes capabilities as core components, as opposed to add-ons, such as compute autoscaling, pod and service networking, application load balancing, cluster DNS, block storage, and GPU support.
--> AWS chooses the configuration automatically, scaled the workload automatically.
--> if you choose AutoMode, AWS cost us very very less for the configurqations we select.


---

Pre-Requisites to Launch EKS cluster.

1. AWS CLI, COnfigure CLI user with Admin Access
2. install EKSCTL
3. Install Kubectl

---



kubectl cluster-info				--> proviodes the cluster info
kubectl cluster-info dump
kubectl get nodes					--> List the nodes associated with cluster
kubectl describe node ip-192-168-3-19.ap-south-1.compute.internal

kubectl get pods

kubectl get all -A					--> Displays all the resources from all namespaces

kubectl get pods,services

kubectl get service,deployment -A

kubectl logs pod/cal-deployment-5dbcc68c56-n5gw5		--> Displays the logs from a pod

kubectl exec -it pod/cal-deployment-5dbcc68c56-n5gw5 -- /bin/sh		--> Connect to a pod

kubectl delete pod/svc/rs <name>									--> To delete the resource

kubectl deploy -f <filename>.yml				--> Do the deployment

kubectl scale deployment cal-deployment --replicas=2			--> we can adjust the deployment properties using command line

kubectl apply -f 03-deployment.yml --dry-run=server 			--> helps to perform dryrun

kubectl top nodes 					--> Displays the resources usage (CPU, Memory)

kubectl port-forward <pod-name> <laptop-port>:<pod-port>		

kubectl get events					--> Displays the cluster events info 

---

Namespace : Helps to logically isolate or seperate the resources in K8s..
--> Multi-Tenant : Multiple teams can use only one cluster without interfering other teams work..
--> Resource limits: we cna setup resource quotas for better workload management..


kubectl get ns

kubectl create ns dev-namespace 			--> Create a Namespace

NAME              STATUS   AGE
default --> our workload runs here. If we run anything without specifying ns explicitly.
kube-node-lease --> manages the node ATT/heartbeat to track node availability.
kube-public --> Publicly readable namespace used for cluster-wide information..
kube-system --> K8s backend/support core/system components runs here.. i.e; CoreDNS, kube-proxy, monitoring..


kubectl get all -n kube-system		--> Display resources from a specific namespace


---

limitranege : Helps to setup the resource limits/quotas.
















